{
 "metadata": {
  "name": "Atlas",
  "signature": "sha256:de71c4ebe6131f4cd2ef84a9728fd5c4a45500cdd5b00c0e0bf485e5fb3912df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Things to be noted. \n",
      "\n",
      "\"\"\"\n",
      "Input string should contain all possible test cases. a) Mentions b) URL c) Emoticons\n",
      "\n",
      "Test cases:\n",
      "-----------\n",
      "\"Hi @Ramki how are you?\"\n",
      "\"Hi, Ramki, How u doing??\"\n",
      "\"Hello (smiles) @ramki , How you doing?\"\n",
      "\"Hello (smiles) @ramki (Coffee)! Where shall we go?\" \n",
      "\"Hey @Bot294i4 (angry) !! Where is my git notification , you idiot(stern face)???\" \n",
      "\"Here is the state of my car!! (sic) https://cars/hummer/{id}/vehicle_state\"\n",
      "\"(Happy)Did you notice the tinyurls?? @Ramki!! (happy)..They are awesome. http://tinyurl.com/mz4k345d\" \n",
      "\"http://www.springerlink.com/link.asp?id=62728, Ramki check this paper!!\"\n",
      "\"(sic)(spoiley)http://www.bing.com/permanent/?request=get-archive&isbn=0342-8244,Ramki!!(Boo Yeah)(Happy)\"\n",
      "\"This is a great site @Mark !! (Smiles) , http://firstsearch.oclc.org/dbname=WorldCat;done=referer;FSIP, get this one as well, the bigger one @Nicholas\n",
      "https://firstsearch.oclc.org/WebZ/FSPrefs?entityjsdetect=:javascript=true:\n",
      "screensize=large:sessionid=fsapp3-36672-f5pysd37-7ipve9:entitypagenum=1:0\".\n",
      "\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'Ramki Subramanian'\n",
      "DEBUG = False  # Flag to set debug level. (print statements on/off)\n",
      "\n",
      "import re\n",
      "import json\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import html\n",
      "from urllib2 import (URLError, HTTPError)\n",
      "import requests\n",
      "from requests.exceptions import SSLError\n",
      "import time\n",
      "from multiprocessing.pool import ThreadPool as Pool\n",
      "\n",
      "\n",
      "class MessageDecode(object):\n",
      "\n",
      "    def __int__(self):\n",
      "        pass\n",
      "\n",
      "    def removeduplicates(self, inputlist):\n",
      "        return list(set(inputlist))\n",
      "\n",
      "    def decode(self, inputstring,messagetime):\n",
      "        \"\"\"\n",
      "\n",
      "        :param inputstring: the input string which needs to be parsed and split\n",
      "        :return: json\n",
      "        \"\"\"\n",
      "\n",
      "        # Dictionary of list of 'mentions', 'emoticons' and 'links'\n",
      "        final_dict = {}\n",
      "\n",
      "        # 1. Find the \"mentions\" from the input string\n",
      "        # Previous method - catches @ only in the start of the word and\n",
      "        # only single word values.\n",
      "        # e = re.findall(r\"(\\B(?:\\@)[\\w\\S]*.*?)\", inputString)\n",
      "        # \\B to catch the @ in the start of the word\n",
      "        # \\S- to handle only single word values\n",
      "        try:\n",
      "            # Advanced method: Match multi-words which starts with '@'\n",
      "            # at any part of the sentence\n",
      "            mentions = re.findall(r\"((?:@)[\\w.]+)\", inputstring)\n",
      "\n",
      "            # add to the final dict only if its not null\n",
      "            if mentions:\n",
      "                mentions_list = [i.strip('@') for i in mentions]\n",
      "                mentions_list = self.removeduplicates(mentions_list)\n",
      "                final_dict['mentions'] = mentions_list\n",
      "        except AssertionError:\n",
      "            pass\n",
      "\n",
      "        # 2. Catch the emoticons from the input string\n",
      "        try:\n",
      "            # \\s for whitespace (handling multiword names like Michael Jackson)\n",
      "            # \\w for alphanumeric (handling user names like mike999)\n",
      "            emoticons = re.findall(r\"(\\([\\w\\s]{1,15}\\).*?)\", inputstring)\n",
      "            if emoticons:\n",
      "                emoticons_list = [em.strip('()') for em in emoticons]\n",
      "                emoticons_list = self.removeduplicates(emoticons_list)\n",
      "                final_dict['emoticons'] = emoticons_list\n",
      "            if DEBUG:\n",
      "                print(\"Emoticons:\",  emoticons)\n",
      "        except AssertionError:\n",
      "            pass\n",
      "\n",
      "        # 3. Picking URL from the chat string\n",
      "        try:\n",
      "            # url_list = re.findall(r\"(http[^\\s]+).*?\",inputString,re.DOTALL)\n",
      "            # ?:  - non groups, www1-9 handles websites with www9 prefix.\n",
      "            url_list = re.findall('(?xi) (?:http[s]?:// | www\\d{0,3}[.] | ftp?:// ) '\n",
      "                                  '(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', inputstring)\n",
      "            url_dict = {}\n",
      "            url_list_of_dicts = []\n",
      "            # pool = Pool(8)\n",
      "            if DEBUG:\n",
      "                print(url_list)\n",
      "            try:\n",
      "                for urlid, each_url in enumerate(url_list):\n",
      "                    try:\n",
      "                        # Get the HTML content of the given web URL\n",
      "                        response = requests.get(each_url.decode('utf-8', 'ignore'))\n",
      "                        # Use BeautifulSoup to parse and extract page title\n",
      "                        # This gave HTTPError sometimes\n",
      "                        soup = BeautifulSoup(response.text)\n",
      "                        title = soup.title.string\n",
      "                        encoded_title = title.encode('ascii','ignore')\n",
      "                    except:\n",
      "                        print \"Could not get the title of this URL\", each_url\n",
      "                        encoded_title = None\n",
      "                    # Use lxml to avoid those errors and its faster than BS.\n",
      "                    # parsed_tree = html.fromstring(response.text)\n",
      "                    # title = parsed_tree.xpath('//title/text()')\n",
      "\n",
      "                    # Create the dict\n",
      "                    url_dict['id'] = urlid\n",
      "                    url_dict['url'] = each_url\n",
      "                    url_dict['title'] = encoded_title\n",
      "\n",
      "                    # Add this list of dicts to the final_dict\n",
      "                    url_list_of_dicts.append(url_dict.copy())\n",
      "                    # .copy() is important else same dict will be copied.\n",
      "                    final_dict['links'] = url_list_of_dicts\n",
      "                    final_dict['message_time'] = messagetime\n",
      "\n",
      "            except (HTTPError, URLError, ValueError, SSLError) as e:\n",
      "                print(\"The server couldn't be reached\")\n",
      "                print('Error code:'), e\n",
      "\n",
      "        except AssertionError:\n",
      "            pass\n",
      "        formatted_json = json.dumps(final_dict, sort_keys=True, indent=4).decode('utf8')\n",
      "\n",
      "        return formatted_json\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    newObj = MessageDecode()\n",
      "    # In Ipython notebook raw_input wont work\n",
      "    #inputString = raw_input(\"Please enter a chat string (Till you press enter)\\n\")\n",
      "    inputString = (\" @Ramki (Smiley) u?? Weather is (awesome)\\n http://www.crummy.com/software/BeautifulSoup/bs4/doc/\"\n",
      "                     \"Did yo u check Adam's blog http://www.imdb.com/title/tt0583452/?ref_=tt_eps_rhs_1 (Coffee) @Ramki @jacob18\"\n",
      "                     \"(Angry )  ftp://ftp6.jp.freebsd.org/pub/FreeBSD/ (Smiley)\")\n",
      "    messageTime = time.ctime()\n",
      "    finaljson = newObj.decode(inputString,messageTime)\n",
      "    print \"###########  Formatted JSON ############ \\n\", finaljson"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Could not get the title of this URL ftp://ftp6.jp.freebsd.org/pub/FreeBSD/\n",
        "###########  Formatted JSON ############ \n",
        "{\n",
        "    \"emoticons\": [\n",
        "        \"Smilaey\", \n",
        "        \"awesome\", \n",
        "        \"Smiley\", \n",
        "        \"angry\", \n",
        "        \"Angry \"\n",
        "    ], \n",
        "    \"links\": [\n",
        "        {\n",
        "            \"id\": 0, \n",
        "            \"title\": \"Smells Like Error 404\", \n",
        "            \"url\": \"http://www.crummy.com/software/BeautifulSoup/bs4/doc/Did\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": 1, \n",
        "            \"title\": \"\\\"Friends\\\" The One Where Everybody Finds Out (TV Episode 1999) - IMDb\", \n",
        "            \"url\": \"http://www.imdb.com/title/tt0583452/?ref_=tt_eps_rhs_1\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": 2, \n",
        "            \"title\": null, \n",
        "            \"url\": \"ftp://ftp6.jp.freebsd.org/pub/FreeBSD/\"\n",
        "        }\n",
        "    ], \n",
        "    \"mentions\": [\n",
        "        \"Ramki\", \n",
        "        \"jacob18\"\n",
        "    ], \n",
        "    \"message_time\": \"Sun Mar 08 19:38:01 2015\"\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}